{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import os.path\n",
    "import os\n",
    "import preprocessing as pp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import featureselection as fs\n",
    "from nltk.tokenize import word_tokenize\n",
    "import datetime\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_normalized_cnb(complement_probs_normalized, vectorized_text, prior_probs):\n",
    "    '''\n",
    "    :param complement_probs: dictionary where key = label and values = dictionary where\n",
    "                            keys = words and values = (# of times word w appears in docs\n",
    "                            NOT labeled l)/(# of words in documents NOT labeled l)\n",
    "    :param idf: dictionary where keys = words and values = (total # docs)/(# of docs in \n",
    "                which we see that word)\n",
    "    :param vectorized_text: words from text that are in valid_words\n",
    "    :param prior_probs: dictionary where keys = labels and values = the probability\n",
    "                        of seeing that label in the dataset\n",
    "    '''\n",
    "    labels = []\n",
    "    freq = Counter(vectorized_text)\n",
    "    for label in prior_probs.keys():\n",
    "        conditional = 0.0\n",
    "        for word in freq.keys():\n",
    "            conditional += (freq[word] * complement_probs_normalized[label][word])\n",
    "        labels.append((label, np.exp(conditional)))\n",
    "    return sorted(labels, key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_naive_bayes(complement_probs, vectorized_text, prior_probs):\n",
    "    '''\n",
    "    :param complement_probs: dictionary where key = label and values = dictionary where\n",
    "                            keys = words and values = (# of times word w appears in docs\n",
    "                            NOT labeled l)/(# of words in documents NOT labeled l)\n",
    "    :param vectorized_text: words from text that are in valid_words\n",
    "    :param prior_probs: dictionary where keys = labels and values = the probability\n",
    "                        of seeing that label in the dataset\n",
    "    '''\n",
    "    labels = []\n",
    "    doc_denom = 0\n",
    "    freq = Counter(vectorized_text)\n",
    "    '''\n",
    "    for word in freq.keys():\n",
    "        for label in prior_probs.keys():\n",
    "            doc_denom += (np.log(prior_probs[label]) + (freq[word]/len(vectorized_text) * complement_probs[label][word]))\n",
    "    print(doc_denom)\n",
    "    '''\n",
    "    for label in prior_probs.keys():\n",
    "        conditional = 0.0\n",
    "        for word in freq.keys():\n",
    "            conditional += (freq[word] * complement_probs[label][word])\n",
    "        labels.append((label, conditional))\n",
    "    return sorted(labels, key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_naive_bayes(conditional_probs, vectorized_text, prior_probs):\n",
    "    '''\n",
    "    :param conditional_probs: dictionary where keys = labels and values = dictionary where\n",
    "                    keys = words and values = P(x|Y)\n",
    "    :param vectorized_text: words from text that are in valid_words\n",
    "    :param prior_probs: dictionary where keys = labels and values = the probability\n",
    "                        of seeing that label in the dataset\n",
    "    '''\n",
    "    labels = []\n",
    "    freq = Counter(vectorized_text)\n",
    "    for label in prior_probs.keys():\n",
    "        conditional = 0.0\n",
    "        for word in vectorized_text:\n",
    "            if conditional_probs[label][word] != 0.0:\n",
    "                conditional += (freq[word] * conditional_probs[label][word])\n",
    "        labels.append((label, np.exp(conditional)))\n",
    "    return sorted(labels, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prior_probabilities(number_labels):\n",
    "    '''\n",
    "    This function will compute the prior probabilities\n",
    "    P(y) = probability of seeing a label with a sample. \n",
    "    Note: since many samples have multiple labels, these prior\n",
    "    probabilites will sum to > 1\n",
    "    :param number_labels: dictionary where keys = number of training sample\n",
    "                            and value = the list of labels associated with it\n",
    "    :return: a dictionary where keys = the label and value = probability of seeing\n",
    "            that label in the document list\n",
    "    '''\n",
    "    prior_probs = defaultdict(float)\n",
    "    i = 0\n",
    "    for num, labels in number_labels.items():\n",
    "        for l in labels:\n",
    "            if not prior_probs[l]:\n",
    "                prior_probs[l] = 1\n",
    "            else:\n",
    "                prior_probs[l] += 1\n",
    "        i += 1\n",
    "    for label, freq in prior_probs.items():\n",
    "        prior_probs[label] /= i\n",
    "    return prior_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(dir_path):\n",
    "    '''\n",
    "    Utility function designed to rename all files in any directory\n",
    "    to a .txt file so they can be read from\n",
    "    :param dir_path: directory of the files to be renamed\n",
    "    '''\n",
    "    for file in os.listdir(dir_path):\n",
    "        filepath = dir_path + '\\\\' + file \n",
    "        os.rename(filepath, filepath+\".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    dir_path = \"C:\\\\Users\\\\ksing\\\\OneDrive\\\\Documents\\\\TextClassifiers\\\\training\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    valid_words = fs.most_useful_features(\"cdmfeatures.txt\")\n",
    "    # valid_words = pp.get_valid_words(dir_path, stop_words)\n",
    "    number_labels_training, number_labels_test = pp.add_labels_to_samples(\"cats2.txt\")\n",
    "    prior_probs = compute_prior_probabilities(number_labels_training)\n",
    "    print(prior_probs.keys())\n",
    "    parameters = pp.get_parameters(dir_path, valid_words, number_labels_training, prior_probs.keys())\n",
    "    \n",
    "    frequencies = parameters[0]\n",
    "    total_frequencies = parameters[1]\n",
    "    idf = parameters[2] \n",
    "    total_word_count_by_label = parameters[3]\n",
    "    total_num_words = parameters[4]\n",
    "\n",
    "    conditional_probs = {label: {word: 0.0 for word in valid_words} for label in prior_probs.keys()}\n",
    "    complement_probs = {label: {word: 0.0 for word in valid_words} for label in prior_probs.keys()}\n",
    "    for label, vector in conditional_probs.items():\n",
    "        denom = total_num_words - total_word_count_by_label[label] + len(valid_words.keys())\n",
    "        for word in vector.keys():\n",
    "            mod_cond_freq = frequencies[label][word] + 1\n",
    "            mod_comp_freq = (total_frequencies[word] - frequencies[label][word]) + 1\n",
    "            conditional_probs[label][word] = np.log(mod_cond_freq/(total_word_count_by_label[label] + len(valid_words.keys())))\n",
    "            complement_probs[label][word] = np.log(mod_comp_freq/denom)\n",
    "\n",
    "    complement_probs_normalized = {label: {word: complement_probs[label][word] for word in valid_words} \n",
    "                                   for label in prior_probs.keys()}\n",
    "    conditional_probs_normalized = {label :{word: 0.0 for word in valid_words} for label in prior_probs.keys()}\n",
    "    normalize_terms = {label: 0.0 for label in prior_probs.items()}\n",
    "    for label, vector in complement_probs.items():\n",
    "        normalize_term_1 = np.sqrt(sum([(complement_probs_normalized[label][word]**2) for word in valid_words]))\n",
    "        normalize_term_2 = np.sqrt(sum([(conditional_probs[label][word]**2) for word in valid_words]))\n",
    "        normalize_terms[label] = normalize_term_1\n",
    "        for word in vector.keys():\n",
    "            complement_probs_normalized[label][word] /= normalize_term_1\n",
    "            conditional_probs_normalized[label][word] = conditional_probs[label][word] / normalize_term_2\n",
    "\n",
    "    # Removing the stemmer actually improves accuracy on test set, who knew\n",
    "    successes, earned, bottom_5,i = 0, 0, 0, 0\n",
    "    computed_label_set = defaultdict(list)\n",
    "    dir_path = \"C:\\\\Users\\\\ksing\\\\OneDrive\\\\Documents\\\\TextClassifiers\\\\test\"\n",
    "    for file in os.listdir(dir_path):\n",
    "        filepath = dir_path + '\\\\' + file \n",
    "        num = int(file[0:len(file) - 4])\n",
    "        text = pp.vectorize_text(valid_words, filepath)\n",
    "        computed_labels = complement_naive_bayes(complement_probs, text, prior_probs)\n",
    "        # computed_labels = multinomial_naive_bayes(conditional_probs_normalized, text, prior_probs)\n",
    "        # computed_labels = weight_normalized_cnb(complement_probs_normalized, text, prior_probs)\n",
    "        suc, e, b5 = pp.accuracy_model(num, number_labels_test, computed_labels)\n",
    "        computed_label_set[num] = [x for x,y in computed_labels]\n",
    "        # MNB with doc length normalization, IDF: 86.10% accuracy (2599.288708513709), 1548 \"Earn\" labels\n",
    "        # CNB with doc length normalization, IDF: 90.02% accuracy(2717.798340548341), 1130 \"Earn\" labels\n",
    "        # However, this approach results in conditional terms that don't make much sense for precision or recall\n",
    "        # WCNB with doc length normalization, IDF: 87.72% accuracy (2648.141955266955), 1542 \"Earn\" labels\n",
    "        \n",
    "        # Perhaps the reason that TF doesn't lead to improvements with this is because we already stripped out the \n",
    "        # stop words, which would be affected the most by this technique\n",
    "\n",
    "        successes += suc\n",
    "        earned += e\n",
    "        bottom_5 += b5\n",
    "        i += 1\n",
    "    print(successes, earned, bottom_5, i)\n",
    "\n",
    "    precision, recall = pp.compute_precision_recall(computed_label_set, number_labels_test, prior_probs.keys())\n",
    "    print(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
